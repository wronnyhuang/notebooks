{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "encoder_classifier",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/wronnyhuang/notebooks/blob/master/encoder_classifier.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "co7MV6sX7Xto",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Universal sentence encoder used for text classification"
      ]
    },
    {
      "metadata": {
        "id": "eAVQGidpL8v5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook illustrates how to access the Universal Sentence Encoder and use it for sentence classification tasks such as negative news.\n",
        "\n",
        "The sentence embeddings can be used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data.\n",
        "\n",
        "We'll demonstrate it on the use case of movie review sentiment from the IMDB dataset (50k labeled examples across train/test). We will train a few neural layers on top of the sentence encoding.\n",
        "\n",
        "The goal of this demonstration is to show that the universal sentence encoder can be used effectively for negative news classification if we train it with enough data."
      ]
    },
    {
      "metadata": {
        "id": "pOTzp8O36CyQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Setup**\n",
        "This section sets up the environment for access to the Universal Sentence Encoder on TF Hub and provides examples of applying the encoder to words, sentences, and paragraphs."
      ]
    },
    {
      "metadata": {
        "id": "lVjNK8shFKOC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the latest Tensorflow version.\n",
        "!pip3 install --quiet \"tensorflow>=1.7\"\n",
        "# Install TF-Hub.\n",
        "!pip3 install --quiet tensorflow-hub\n",
        "!pip3 install --quiet seaborn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MSeY-MUQo2Ha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwty8Z6mAkdV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NA-3kaEJQiKR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now load tensorflow hub's module for the universal sentence encoder and name it ```embed```.\n"
      ]
    },
    {
      "metadata": {
        "id": "W7j81dqPvMFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "03b2a1ae-1305-4cdd-d84b-42e9064b966d"
      },
      "cell_type": "code",
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "tf.reset_default_graph()\n",
        "embed = hub.Module(module_url)\n",
        "\n",
        "# Compute a representation for each message, showing various lengths supported.\n",
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how lon the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the embedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "# Reduce logging output.\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  message_embeddings = session.run(embed(messages))\n",
        "\n",
        "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "    print(\"Message: {}\".format(messages[i]))\n",
        "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "    message_embedding_snippet = \", \".join(\n",
        "        (str(x) for x in message_embedding[:3]))\n",
        "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n",
            "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n",
            "Message: Elephant\n",
            "Embedding size: 512\n",
            "Embedding: [0.044984713196754456, -0.05743394419550896, 0.002211472485214472, ...]\n",
            "\n",
            "Message: I am a sentence for which I would like to get its embedding.\n",
            "Embedding size: 512\n",
            "Embedding: [0.05568017065525055, -0.009607895277440548, 0.006246253382414579, ...]\n",
            "\n",
            "Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how lon the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\n",
            "Embedding size: 512\n",
            "Embedding: [0.04193407669663429, 0.08135093003511429, -0.0026269867084920406, ...]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q8F4LNGFqOiq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "bfcec16f-c31e-4f62-9f43-60fdd2391e7a"
      },
      "cell_type": "code",
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n",
            "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/3'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YNgzKk9uaWQm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load and embed dataset\n",
        "***Only need to do this once per session!!***\n",
        "*Skip to next section if already done*"
      ]
    },
    {
      "metadata": {
        "id": "ElHT44UoQ-XU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download and untar the IMDB dataset"
      ]
    },
    {
      "metadata": {
        "id": "prn6-VOMMXwa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "973c2681-ae7f-4161-b7f2-8f253249ae08"
      },
      "cell_type": "code",
      "source": [
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xzf aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V1RJdtr9ZQ2a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Extract text from files to a list of strings and save that list"
      ]
    },
    {
      "metadata": {
        "id": "u21uNvV5MA6J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get list of filenames\n",
        "data_root = 'aclImdb'\n",
        "trainposfiles = glob(os.path.join(data_root, 'train', 'pos', '*.txt'))\n",
        "trainnegfiles = glob(os.path.join(data_root, 'train', 'neg', '*.txt'))\n",
        "testposfiles = glob(os.path.join(data_root, 'test', 'pos', '*.txt'))\n",
        "testnegfiles = glob(os.path.join(data_root, 'test', 'neg', '*.txt'))\n",
        "\n",
        "# initialize list of training/test data which we'll load all into memory\n",
        "xtrain = []\n",
        "ytrain = []\n",
        "xtest = []\n",
        "ytest = []\n",
        "\n",
        "# loop through all training/test files and store the sentences/labels\n",
        "for file in trainposfiles:\n",
        "  with open(file) as f:\n",
        "    xtrain.append(f.read())\n",
        "    ytrain.append(1.) # 1 for positive sentiment, 0 for negative\n",
        "\n",
        "for file in trainnegfiles:\n",
        "  with open(file) as f:\n",
        "    xtrain.append(f.read())\n",
        "    ytrain.append(0.)\n",
        "\n",
        "for file in testposfiles:\n",
        "  with open(file) as f:\n",
        "    xtest.append(f.read())\n",
        "    ytest.append(1.)\n",
        "\n",
        "for file in testnegfiles:\n",
        "  with open(file) as f:\n",
        "    xtest.append(f.read())\n",
        "    ytest.append(0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CzodBC1PZZfm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Convert the messages to sentence embeddings using the encoder. Then save the resulting embeddings of all the train/test data into a pickle file\n",
        "**This cell takes a while to execute, also must run on CPU (OOM error on GPU)**"
      ]
    },
    {
      "metadata": {
        "id": "uxnHMc8fm64C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert the messages (strings) into sentence embeddings using the encoder\n",
        "with tf.Session() as sess:\n",
        "  sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  \n",
        "  # train - embed sentences TAKES A WHILE\n",
        "  xtrain_emb = np.empty((0,512), float)\n",
        "  batch_size = 500\n",
        "  n_batch = int(len(xtrain)/batch_size)\n",
        "  for i in range(n_batch):\n",
        "    batch = xtrain[(batch_size*i):(batch_size*(i+1))]\n",
        "    batch_emb = sess.run(embed(batch))\n",
        "    xtrain_emb = np.append(xtrain_emb, batch_emb, axis=0)\n",
        "    print(i, n_batch, xtrain_emb.shape)\n",
        "  ytrain = np.array(ytrain).reshape(-1,1) # reshape ytrain\n",
        "  with open('train_embeddings.pkl', 'wb') as f: # save to disk\n",
        "    pickle.dump((xtrain, xtrain_emb, ytrain), f)\n",
        "\n",
        "  # test - embed sentences TAKES A WHILE\n",
        "  xtest_emb = np.empty((0,512), float)\n",
        "  batch_size = 500\n",
        "  n_batch = int(len(xtest)/batch_size)\n",
        "  for i in range(n_batch):\n",
        "    batch = xtest[(batch_size*i):(batch_size*(i+1))]\n",
        "    batch_emb = sess.run(embed(batch))\n",
        "    xtest_emb = np.append(xtest_emb, batch_emb, axis=0)\n",
        "    print(i, n_batch, xtest_emb.shape)\n",
        "  ytest = np.array(ytest).reshape(-1,1) # reshape ytest\n",
        "  with open('test_embeddings.pkl', 'wb') as f: # save to disk\n",
        "    pickle.dump((xtest, xtest_emb, ytest), f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTnVkBaH3QpB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Optional: upload the pickle files to dropbox"
      ]
    },
    {
      "metadata": {
        "id": "gCxnz8mYzuCZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setup dropbox access with a token\n",
        "! curl \"https://raw.ontent.com/andreafabrizi/Dropbox-Uploader/master/dropbox_uploader.sh\" -o /bin/dropbox_uploader.sh\n",
        "! chmod +x /bin/dropbox_uploader.sh\n",
        "! dropbox_uploader.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-M2YzmZb3jPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f1a4487b-8622-4b7c-9f18-946d772ddc47"
      },
      "cell_type": "code",
      "source": [
        "# upload pickles to dropbox\n",
        "! dropbox_uploader.sh upload t*_embeddings.pkl datasets/imdb_universal_sentence/\n",
        "! dropbox_uploader.sh -q share datasets/imdb_universal_sentence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " > Uploading \"/content/test_embeddings.pkl\" to \"/datasets/imdb_universal_sentence/test_embeddings.pkl\"... DONE\n",
            "> Skipping file \"/content/train_embeddings.pkl\", file exists with the same hash\n",
            "https://www.dropbox.com/sh/86u0243mq6fhv2z/AAAJebdtVG5fgLuZVx6BzzREa?dl=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YSgZf6LYavL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train neural net on top of embedding layer"
      ]
    },
    {
      "metadata": {
        "id": "ixV_N5Re3uWr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First download pickle files saved from previous section"
      ]
    },
    {
      "metadata": {
        "id": "3NbHR8sHXxct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4c32f68e-192b-4107-8ce8-33cec1c172b1"
      },
      "cell_type": "code",
      "source": [
        "! curl -L https://www.dropbox.com/sh/86u0243mq6fhv2z/AAAJebdtVG5fgLuZVx6BzzREa?dl=0 > tmp.zip\n",
        "! unzip tmp.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  147M    0  147M    0     0   9.8M      0 --:--:--  0:00:15 --:--:-- 11.0M\n",
            "Archive:  tmp.zip\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            "  inflating: train_embeddings.pkl    \n",
            "  inflating: test_embeddings.pkl     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nQ2KVa7BaPDB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the embeddings of train/test data from pickle file"
      ]
    },
    {
      "metadata": {
        "id": "em_ep6LaZsYv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load train/test embeddings to memory\n",
        "with open('train_embeddings.pkl', 'rb') as f:\n",
        "  xtrain, xtrain_emb, ytrain = pickle.load(f)\n",
        "with open('test_embeddings.pkl', 'rb') as f:\n",
        "  xtest, xtest_emb, ytest = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktSgS8VZClLj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get tensorboard running so we can visualize our results while training"
      ]
    },
    {
      "metadata": {
        "id": "T_xlVuIxCjjg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "799bf371-a898-47be-c1ba-8f60696c1ccd"
      },
      "cell_type": "code",
      "source": [
        "# tensorboard\n",
        "! git clone https://github.com/wronnyhuang/bin\n",
        "! mv bin/* /bin/ && rm -r bin\n",
        "! sh /bin/install_ngrok.sh\n",
        "! sh /bin/tboard.sh . n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bin'...\n",
            "remote: Counting objects: 58, done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 58 (delta 20), reused 49 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (58/58), done.\n",
            "downloading ngrok\n",
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: /root/bin/ngrok         \n",
            "tensorboarding directory .\n",
            "tunneling with ngrok\n",
            "nohup: appending output to 'nohup.out'\n",
            "nohup: appending output to 'nohup.out'\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XQYcqte8Tqnj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Add a neural network on top of the embedding layer with ```len(n_hidden)``` layers and ```n_hidden[i]``` hidden units in the ```i```th layer"
      ]
    },
    {
      "metadata": {
        "id": "kR5i_24O7hp4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_hidden = [768, 1024]\n",
        "\n",
        "# reset graph\n",
        "tf.reset_default_graph()\n",
        "embed = hub.Module(module_url)\n",
        "\n",
        "# input placeholders\n",
        "xinput = tf.placeholder(dtype=tf.float32, shape=[None, xtrain_emb.shape[1]])\n",
        "yinput = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
        "linput = tf.placeholder(dtype=tf.float32) # lrn rate\n",
        "tinput = tf.placeholder(dtype=tf.bool) # training mode flag\n",
        "\n",
        "# add neural network layers\n",
        "z = xinput\n",
        "for nh in n_hidden:\n",
        "  z = tf.layers.dense(inputs=z, units=nh, use_bias=False)\n",
        "#   z = tf.layers.batch_normalization(z, training=tinput)\n",
        "  z = tf.nn.relu(z)\n",
        "  z = tf.layers.dropout(z, rate=0.5, training=tinput)\n",
        "\n",
        "# logit layer\n",
        "logits = tf.layers.dense(inputs=z, units=1)\n",
        "\n",
        "# binary cross entropy\n",
        "xent = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=yinput)\n",
        "xent = tf.reduce_mean(xent)\n",
        "\n",
        "# weight decay\n",
        "wdec = tf.add_n([tf.nn.l2_loss(t) for t in tf.trainable_variables()])\n",
        "\n",
        "# total loss with regularizers\n",
        "criterion = xent + 5e-4*wdec\n",
        "\n",
        "# step operation for the optimizer\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "with tf.control_dependencies(update_ops):\n",
        "#   train_step = tf.train.MomentumOptimizer(learning_rate=linput, momentum=.9).minimize(criterion)\n",
        "#   train_step = tf.train.GradientDescentOptimizer(learning_rate=linput).minimize(criterion)\n",
        "  train_step = tf.train.AdamOptimizer(learning_rate=linput).minimize(criterion)\n",
        "\n",
        "# prediction and accuracy\n",
        "prediction = tf.nn.sigmoid(logits)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(yinput, tf.round(prediction)), tf.float32))\n",
        "\n",
        "# adding some operations for tensorboard visualization\n",
        "tf.summary.scalar('accuracy', accuracy)\n",
        "tf.summary.scalar('loss', criterion)\n",
        "merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hx2OoXMSXhR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Having built our static computational graph, we can now train it by feeding in some data and doing backprop. We'll run the training for ```n_epoch``` epochs with a batch size of ```batch_size```"
      ]
    },
    {
      "metadata": {
        "id": "JTosP1qPWlke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "cd71745e-d3a3-4214-e6dd-bb29e111fda4"
      },
      "cell_type": "code",
      "source": [
        "n_epoch = 50\n",
        "batch_size = 100\n",
        "\n",
        "# prepare for minibatching\n",
        "np.random.seed(1)\n",
        "x_size = len(xtrain_emb)\n",
        "n_batch = int(x_size/batch_size)\n",
        "\n",
        "# prepare session\n",
        "sess = tf.Session()\n",
        "sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "\n",
        "log_root = '512-'+'-'.join([str(nh) for nh in n_hidden])+'-1'\n",
        "train_writer = tf.summary.FileWriter('./'+log_root+'/train', sess.graph)\n",
        "test_writer = tf.summary.FileWriter('./'+log_root+'/test', sess.graph)\n",
        "\n",
        "# train and test iteratively\n",
        "global_step = 1\n",
        "for i in range(n_epoch): # loop over epochs\n",
        "\n",
        "  # perform test run\n",
        "  summaries_test, acc, loss, ypred = sess.run([merged, accuracy, criterion, prediction],\n",
        "                       feed_dict={xinput:xtest_emb, yinput:ytest, tinput:False})\n",
        "  test_writer.add_summary(summaries_test, global_step)\n",
        "  print('TEST: epoch='+str(i)+' acc='+str(acc)+' loss='+str(loss))\n",
        "\n",
        "  # learning rate schedule\n",
        "  if i<30: lr = 1e-2\n",
        "  elif i<45: lr = 1e-3\n",
        "  elif i<55: lr = 1e-4\n",
        "\n",
        "  # perform training runs\n",
        "  for j in range(n_batch): # loop over minibatches\n",
        "\n",
        "    # acquire minibatches\n",
        "    randidx = np.random.permutation(len(xtrain))\n",
        "    batchidx = randidx[(j*batch_size):((j+1)*batch_size)]\n",
        "    xbatch = xtrain_emb[batchidx]\n",
        "    ybatch = ytrain[batchidx]\n",
        "\n",
        "    # run backprop\n",
        "    summaries, _, _ = sess.run([merged, train_step, update_ops],\n",
        "                            feed_dict={xinput:xbatch, yinput:ybatch, linput:lr, tinput:True})    \n",
        "    train_writer.add_summary(summaries, global_step)\n",
        "    global_step += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST: epoch=0 acc=0.48636 loss=1.0686116\n",
            "TEST: epoch=1 acc=0.8314 loss=0.48076802\n",
            "TEST: epoch=2 acc=0.81812 loss=0.5111362\n",
            "TEST: epoch=3 acc=0.82536 loss=0.47929093\n",
            "TEST: epoch=4 acc=0.8404 loss=0.46798676\n",
            "TEST: epoch=5 acc=0.83632 loss=0.46586758\n",
            "TEST: epoch=6 acc=0.83692 loss=0.48687893\n",
            "TEST: epoch=7 acc=0.83616 loss=0.484956\n",
            "TEST: epoch=8 acc=0.83676 loss=0.4545849\n",
            "TEST: epoch=9 acc=0.831 loss=0.49194786\n",
            "TEST: epoch=10 acc=0.84068 loss=0.47417253\n",
            "TEST: epoch=11 acc=0.83168 loss=0.5018512\n",
            "TEST: epoch=12 acc=0.83232 loss=0.4881341\n",
            "TEST: epoch=13 acc=0.83648 loss=0.47108844\n",
            "TEST: epoch=14 acc=0.83384 loss=0.47873434\n",
            "TEST: epoch=15 acc=0.83792 loss=0.47408855\n",
            "TEST: epoch=16 acc=0.83836 loss=0.46277648\n",
            "TEST: epoch=17 acc=0.83872 loss=0.45995894\n",
            "TEST: epoch=18 acc=0.83672 loss=0.46479803\n",
            "TEST: epoch=19 acc=0.83088 loss=0.48478538\n",
            "TEST: epoch=20 acc=0.8292 loss=0.47762442\n",
            "TEST: epoch=21 acc=0.8218 loss=0.49071473\n",
            "TEST: epoch=22 acc=0.8402 loss=0.50834334\n",
            "TEST: epoch=23 acc=0.82964 loss=0.48856035\n",
            "TEST: epoch=24 acc=0.83736 loss=0.4678517\n",
            "TEST: epoch=25 acc=0.8256 loss=0.4846193\n",
            "TEST: epoch=26 acc=0.83236 loss=0.48270005\n",
            "TEST: epoch=27 acc=0.83472 loss=0.47486514\n",
            "TEST: epoch=28 acc=0.83368 loss=0.47205192\n",
            "TEST: epoch=29 acc=0.8356 loss=0.4702142\n",
            "TEST: epoch=30 acc=0.8322 loss=0.47072327\n",
            "TEST: epoch=31 acc=0.8412 loss=0.40258396\n",
            "TEST: epoch=32 acc=0.84204 loss=0.38871217\n",
            "TEST: epoch=33 acc=0.84176 loss=0.3838319\n",
            "TEST: epoch=34 acc=0.84236 loss=0.38107562\n",
            "TEST: epoch=35 acc=0.84348 loss=0.37664372\n",
            "TEST: epoch=36 acc=0.84336 loss=0.37404743\n",
            "TEST: epoch=37 acc=0.8434 loss=0.3733203\n",
            "TEST: epoch=38 acc=0.84292 loss=0.3712844\n",
            "TEST: epoch=39 acc=0.84372 loss=0.37291753\n",
            "TEST: epoch=40 acc=0.84468 loss=0.36807206\n",
            "TEST: epoch=41 acc=0.84512 loss=0.3704374\n",
            "TEST: epoch=42 acc=0.84568 loss=0.36801443\n",
            "TEST: epoch=43 acc=0.844 loss=0.37047923\n",
            "TEST: epoch=44 acc=0.84456 loss=0.3690205\n",
            "TEST: epoch=45 acc=0.847 loss=0.3669637\n",
            "TEST: epoch=46 acc=0.84604 loss=0.36656195\n",
            "TEST: epoch=47 acc=0.84536 loss=0.36584836\n",
            "TEST: epoch=48 acc=0.84508 loss=0.36565888\n",
            "TEST: epoch=49 acc=0.8456 loss=0.3653592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4sOIZcA7e0vj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Save trained model to disk"
      ]
    },
    {
      "metadata": {
        "id": "djzUYW6meOeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# saving trained weights to disk\n",
        "log_root = '512-'+'-'.join([str(nh) for nh in n_hidden])+'-1'\n",
        "saver = tf.train.Saver()\n",
        "save_path = saver.save(sess, os.path.join(log_root,'model.ckpt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D1bupwYmfB5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "I-08Sf13xxSa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Take a look at a random sampling of the test results"
      ]
    },
    {
      "metadata": {
        "id": "lDdrtg0LR2YQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "a534e180-8f3b-463c-da3f-2c6a156fed63"
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(2)\n",
        "idx = np.random.permutation(xtest_emb.shape[0])[:5]\n",
        "for i in idx:\n",
        "  print('Sample '+str(i)+' | truth='+str(ytest[i])+' prediction='+str(ypred[i]))\n",
        "  print(xtest[i]+'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample 15050 | truth=[0.] prediction=[0.0742907]\n",
            "this movie is allegedly a comedy.so where did all the laughs go.did the forget to put them in,on the version i watched.as a football movie,it is mildly entertaining,i guess.maybe'm just a stick in the mud,with no discernible sense of humour.or maybe this movie just isn't funny.it is also annoying,with that way over the top \"you're a winner\"musical score.and the odd thing is,the team sucked through most of the season,only winning the last two games,and the last game meant nothing since they were not in the playoffs.so what is the point? are they celebrating mediocrity?I don't see it.if anybody knows,please let me know.anyway,this movie isn't great or even very good.i'm giving it a low 3*\n",
            "\n",
            "Sample 9386 | truth=[1.] prediction=[0.99091774]\n",
            "Ray is one of those movies that makes you pause. You actually think about what you heard or think about what you read about this man and it doesn't even come close. During my first viewing of Ray I forgot I was watching a movie I felt like a peeping tom watching this man's life thru a window. This movie is so compelling it drags you in and it involves your every emotion you go thru a emotional roller-coaster ride and when it's over you don't want to do it again so soon because it has that kind of emotional punch that other movies are lacking. Jamie Foxx deserved his Oscar and quite rightfully so his performance is spectacular and it should be held up as the standard for anybody wanting to do a bio pic anytime soon. This movie is as good as it's subject both deserved the titles classic and legend.\n",
            "\n",
            "Sample 486 | truth=[1.] prediction=[0.9622592]\n",
            "This is an action packed film that makes me feel very peaceful and relaxed every time I see it. The film (short of its conclusion) demonstrates that in the face of extreme odds, it is still possible to prevail.<br /><br />This film is very refreshing, and likely to be banned at any moment. Get a copy of it before the thought police burn every copy they can find. They don't want you to have hope for the future, or to think you have a chance.<br /><br />On the other hand, should Political Correctness fail to supress it, this would be an excellent movie to release on DVD. Such a release could contain interviews with the writer and director, and related goodies. I'm sure it would sell some copies, and I would be one of the first to buy it.<br /><br />- Mincka\n",
            "\n",
            "Sample 12924 | truth=[0.] prediction=[0.03963554]\n",
            "Oh f*cking hell, where should I start... First of all; this show is just another stupid American non-funny so called comedy which has pathetic acting and very very poor humor. The American way of laughing-track business makes the whole thing even worse. How come I can hear laughter, yet there's nothing funny happening? Pretty stupid, eh? This show is only for those American people who haven't ever heard that there are far more funnier, better and wittier comedies - not only in Great Brittain, but also in America (The Simpsons for example). I simply can't understand what is so good about \"Reba\" that it has lasted for long a while in television. It has nothing new to offer, it underestimates the (possible) viewers in so many ways and it simply isn't funny at all. I could have lived with the fact that there are so bad shows as \"Reba\", but why the hell they had to run it here in Finland. If I see few seconds of this horrible show the rest of the day is ruined for me. Take my word and believe me - this show sucks ass even more than these kind of American \"comedies\" usually does. This is simply horrible. Do yourself a favor; don't ever watch this peace of sh*t. <br /><br />Well I leave the commenting for those who now this language better. Thanks for your (possible) interest.\n",
            "\n",
            "Sample 16897 | truth=[0.] prediction=[0.20321882]\n",
            "Ghost Story has an interesting feminist revenge tale premise, A-list veteran actors, colorful flashbacks with nifty look-a-like youthful counterparts of the old men. scary staccato music heralding the approaching horrors, atmospheric New England winter weather, and an excellent charismatic actress in the title role. Ghost Story could have been much more effective in black and white and in eliminating some of the more lurid special effects, and to presenting a more cogent screenplay (we should not have to be wondering about why the two trailer-parkish acolytes are in the script) The biggest detriment of the film is Craig Wassan (definitely separated at birth from Bill Maher) who from perhaps editing or just bad acting, is totally ineffective. He seems to \"specialize\" in wide-eyed, wide-mouthed reaction shots; not a lot of personality here. The revelation however is Alice Krige, pale-faced, enigmatic, terrifying underneath the placid exterior. However, her Eva Galli is creepy even before she meets her fate; I mean, a young woman who says things like \"I'd like to take a bite out of you\" or \"Dance with me, you little toad!\" is already not in the land of the living. Ghost Story would have been much better in a low-key, Val Lewton mode. The overdone special effects completely undercut the chill factor.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I8vLwdVzxt_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look also at the errors"
      ]
    },
    {
      "metadata": {
        "id": "geGsVKtQaZJX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "70a46d16-81b1-40de-f532-a821a8e4290b"
      },
      "cell_type": "code",
      "source": [
        "ypred_ = np.round(ypred)\n",
        "false_positive = [(x,yp,yt) for x,yp,yp_,yt in zip(xtest,ypred,ypred_,ytest) if yp_==1 and yt==0]\n",
        "false_negative = [(x,yp,yt) for x,yp,yp_,yt in zip(xtest,ypred,ypred_,ytest) if yp_==0 and yt==1]\n",
        "\n",
        "print('False positives:\\n')\n",
        "idx = np.random.permutation(len(false_positive))[:5]\n",
        "for i in idx:\n",
        "  print('Sample '+str(i)+' | truth='+str(false_positive[i][2])+' prediction='+str(false_positive[i][1]))\n",
        "  print(false_positive[i][0]+'\\n')\n",
        "  \n",
        "print('False negatives:\\n')\n",
        "idx = np.random.permutation(len(false_negative))[:5]\n",
        "for i in idx:\n",
        "  print('Sample '+str(i)+' | truth='+str(false_negative[i][2])+' prediction='+str(false_negative[i][1]))\n",
        "  print(false_negative[i][0]+'\\n')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False positives:\n",
            "\n",
            "Sample 236 | truth=[0.] prediction=[0.8386168]\n",
            "If it wasn't for the terrific music, I would not hesitate to give this cinematic underachievement 2/10. But the music actually makes me like certain passages, and so I give it 5/10.\n",
            "\n",
            "Sample 547 | truth=[0.] prediction=[0.6503953]\n",
            "I am astonished at the major comments here for this OK surf film. It really stems from the \"California Dreamin\" school of barnyard to beach antics and isn't really plausible. The idea that the lead kid learn to ride a board SO well in a concrete wave pool that he beats the real surfers at their game in the real ocean, is just plain silly. In Australia where most urban teens do surf, this film was laughed at audiences took it all with a grain of sea salt. Made in the 80s but with its heart in the 60s, it is fun to watch and looks and sounds good, but it is not a in a classic class at all. Even the actors didn't outlast this. We're seriously in LIQUID BRIDGE or RIDE THE WILD SURF or BEACH BLANKET BINGO land here. Oz stars like Occy and BIG Wednesday hero Gerry Lopez are drafted in to add head nodding recognition to our farm boy's wave prowess, but it only made the crowd in the cinema guffaw. It is for 10 years olds who do not question much. It's not even IN GODS HANDS and that was silly too.\n",
            "\n",
            "Sample 1761 | truth=[0.] prediction=[0.69924414]\n",
            "Attractive husband and wife writing team Robert Wagner (as Joel Gregory) and Kate Jackson (as Donna Gregory) arrive at the spooky mansion of actress \"Lorna Love\" (actually, silent film star Harold Lloyd's house). Mr. Wagner and Ms. Jackson are contracted to write the silent movie star's biography. Wagner has a personal interest in the project, since his father was once the famed star's lover. Mysterious events unfold, and Jackson must fight to save her husband from the spirit of the beautiful blonde, who is \"perfectly preserved\" in a crypt on the estate; moreover, the evil woman seems bent on possessing her husband, and murdering Jackson! <br /><br />This is very much a \"Night of Dark Shadows\" variation, co-starring genuine \"Dark Shadows\" alumni Kate Jackson, who knows and plays her part well. Robert Wagner lacks David Selby's intensity. Sylvia Sidney (as Mrs. Josephs) sidesteps Grayson Hall. Marianna Hill is not a match for Lara Parker (or Diana Millay). Bill Macy (as Oscar Payne) is good in a part that would have been played by John Karlen (in a Dan Curtis production).<br /><br />There are smooth cameos by Joan Blondell, John Carradine, and Dorothy Lamour. Ms. Lamour's delivery resembles Joan Bennett, which begs the question: why didn't producer Aaron Spelling get more of the original \"Dark Shadows\" regulars? <br /><br />Director E.W. Swackhamer was Bridget Hanley's husband; he worked with Ms. Blondell on \"Here Come the Brides\", and with Jackson on \"The Rookies\". \"Death at Love House\" has, arguably, a tighter storyline than the \"Night of Dark Shadows\" film; it differs in the movie star angle; and, in its \"Father Eternal Fire\" ending, it more closely resembles the TVseries' \"Laura the Phoenix\" storyline. <br /><br />**** Death at Love House (9/3/76) E.W. Swackhamer ~ Robert Wagner, Kate Jackson, Sylvia Sidney\n",
            "\n",
            "Sample 891 | truth=[0.] prediction=[0.6356417]\n",
            "I usually enjoy watching Laurel and Hardy, but this is obviously one of the films they made while they were on their way to becoming a successful comedy team.<br /><br />The plot is all too simple, and is mainly based on one joke; how strange kilts and Scotsmen are. And that's all. Okay, there are some other jokes, but I didn't find them very funny at all; they are outdated and (I guess) were not very entertaining when the movie was first released.<br /><br />Still, the movie has got two of the most charming faces in history, and they make the best out of the awkward story (which I expect was filmed without a proper script) and the scenery is nice to look at. <br /><br />In my opinion, watching this is only worthwhile for Laurel and Hardy fans, other people should stay away from it.\n",
            "\n",
            "Sample 631 | truth=[0.] prediction=[0.6014598]\n",
            "After hoo-hooing American Indians scalp number one son, frontiersman Bruce Bennett (as Daniel Boone) seems, at first, like he wants to get even; but, he really wants to make friends with the natives. When sad-eyed Indian chief Lon Chaney Jr. (as Blackfish) also loses number one son, it gets more difficult to clear up misunderstandings. Apparently, this was Republic Pictures' attempt to do for their \"Daniel Boone, Trail Blazer\" what Disney Studio's had successfully done with \"Davy Crockett, King of the Wild Frontier\" (1955).<br /><br />The \"Dan'l Boone\" song, whistled and sung by a group of children in a wagon, did not follow Fess Parker's \"Davy Crockett\" up the Hit Parade. Singer Faron Young (as Faron Callaway) doesn't perform the title song (perhaps wisely); he does sing \"Long Green Valley\", and makes a good impression as a blond boyfriend for Boone's daughter. But, Spanish actor Freddy Fernandez is the film's most valuable player. In a cute scene, Mr. Fernandez reminds Mr. Young the name of the character (\"Susannah\") he is supposed to be in love with.<br /><br />**** Daniel Boone, Trail Blazer (10/5/56) Ismael Rodríguez ~ Bruce Bennett, Lon Chaney Jr., Faron Young, Freddy Fernandez\n",
            "\n",
            "False negatives:\n",
            "\n",
            "Sample 1540 | truth=[1.] prediction=[0.2661116]\n",
            "PEOPLE ARE STUPID.You shouldn't cheer when Paris gets killed in the head with a pipe! It is plain rude!What did Paris do to you? What if that was you? You wouldn't want people cheering for your death!Anyway it was really gory, which I liked and it was cool when Elisha's finger gets cut off.It was weird with that twist that they didn't have 2 sons they had tree and all, It makes way for a House of Wax 2: Ressurction or something.Paris was great acting in the chase scene.Elisha and Chad chemistry is great, they deserve an Oscar.My friends and I were cracking up when that guy said that's hot when Paris and the guy from Cousin Skeeter making out.Lol.K bye.\n",
            "\n",
            "Sample 1538 | truth=[1.] prediction=[0.32492903]\n",
            "The End Of Suburbia (TEOS) is a very useful film. It's also important and provocative. There seems to be no middle ground with either the film or its main source of entertainment, the anti-sprawl Meister, James Howard Kunstler. <br /><br />While I am not a big fan of the New Urbanism, my criticism of it is because of its small vision. In the case of New Urbanist Peter Calthorpe - another talking head - you finally hear what's somewhat obvious in and amongst the special added TEOS out-takes... Calthorpe just doesn't understand peak oil. <br /><br />I've used this as a teaching tool in economics classes to get at the importance of land as a factor of production - a fact long diminished by Neoclassical Economics - and also as a vehicle for educating about: peak oil, our wastrel land use, global warming, our threatened food production, public transit our compromised future<br /><br />Move over South Park! .... Made by Canadians from Toronto for $25,000 and released in May 2004, this video sold over 24,000 copies by October 2005. One major DVD rental vendor recently ordered almost 400 more copies.<br /><br />The End Of Suburbia sales were actually climbing 1 1/2 years after its release and it has also been available on one of the major online video services since September 2005.<br /><br />A sequel, Escape From Suburbia, is in the works with a possible release by August 2006.\n",
            "\n",
            "Sample 1260 | truth=[1.] prediction=[0.19776624]\n",
            "I rank OPERA as one of the better Argento films. Plot holes and inconsistencies? Sure, but I don't think they impair this film as much as many other reviewers seem to. A lot of elements that are in many of Argento's films are kinda \"off-the-wall\", but that's part of the draw of his films...<br /><br />Short story: psycho stalks the opera's new leading lady. The typical Argento twists and turns ensue, leading up to a decent payoff of a climax. Not Argento's best, but I still pull this one out from time to time. Definitely worth a look if you like his other stuff - just don't get this one mixed up with the abysmal PHANTOM OF THE OPERA remake that Argento did, that one is truly awful... 8/10\n",
            "\n",
            "Sample 346 | truth=[1.] prediction=[0.45712453]\n",
            "While possibly the stupidest, most tasteless, and violent slapstick comedy ever made, Guest House is also a very funny one. Don't listen to the critics, they have no sense of humour. While the climax runs out of steam (but not vomit), it's still a funny party movie. Seven candles in the eye out of ten.\n",
            "\n",
            "Sample 740 | truth=[1.] prediction=[0.16110526]\n",
            "*The whereabouts of Al Capone<br /><br />*Who shot JFK?<br /><br />*Cynthia Gibb lands the part of \"Gypsy\" in the TV remake<br /><br />These are some of the great unsolved mysteries of the 20th century. How else can I say it, except, I thought she was unredeemingly awful. Mannequin mannerisms, poor reactionary acting (ie: that blank, stoic stare while he co-star in the scene speaks)and a singing voice that most voice coaches would rate \"mediocre\". But she is stunningly gorgeous and after all, wasn't that what the Gypsy character is all about? Cashing in on her looks cuz' she didn't cut the mustard in the talent department?<br /><br />As for the rest... Bette is fantastic. Whether or not she's playing herself or playing Mama Rose, it works either way, and I for one thought Rosalind Russell was as exciting as drywall in the original. Peter Riegart as \"Herbie\" is the perfect understated foil to Bette's over-the-top Mama, and he's the medium-temperature porridge between Midler's hot dish and Gibb's stone cold mush. Riegart is juuuust right.<br /><br />One final holler to the man responsible for decades to come of Cher jokes: Bob Mackie. Drag queens would kill for the glitz and glamour on display here. Everything's coming up sequins and bugle beads!<br /><br />\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "74HBRQRoQ16-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try our own paragraphs"
      ]
    },
    {
      "metadata": {
        "id": "KbYQk5wlv_ys",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "d602d1f9-e341-4338-d975-8a7ee8bb9f1d"
      },
      "cell_type": "code",
      "source": [
        "xinfer = ['i did like this movie',\n",
        "          'i did not like this movie',\n",
        "          'this movie was not good',\n",
        "          'this movie was not bad',\n",
        "          'i loved nothing in this movie',\n",
        "          'i loved everything in this movie',\n",
        "          'i hated nothing in this movie',\n",
        "          'i hated everything in this movie',\n",
        "          'tom thought this was a great movie, i disagree with him',\n",
        "          'tom thought this was a great movie, i agree with him',\n",
        "          'tom thought this was a great movie',\n",
        "          'this was not my favorite. the story was good but the graphics left something to be desired',\n",
        "          'it seems like every time i go to the theater this movie has the most people. when i finally decided to go watch it, i realized what i was missing out on',\n",
        "          'this was my favorite. the story was good and the graphics were a force to be reckoned with',\n",
        "          'people have said this was not so bad, they were not kidding',\n",
        "          'this movie was okay but i prefer something else'\n",
        "         ]\n",
        "\n",
        "# inference thru model\n",
        "xinfer_emb = sess.run(embed(xinfer))\n",
        "\n",
        "# xinfer_emb = sess.run(embed(xinfer))\n",
        "ypred = sess.run(prediction, feed_dict={xinput:xinfer_emb, tinput:False})\n",
        "\n",
        "# display results\n",
        "for x,y in zip(xinfer,ypred):\n",
        "  print('Prediction: '+str(y))\n",
        "  print(x+'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [0.93625546]\n",
            "i did like this movie\n",
            "\n",
            "Prediction: [0.0171352]\n",
            "i did not like this movie\n",
            "\n",
            "Prediction: [0.00364026]\n",
            "this movie was not good\n",
            "\n",
            "Prediction: [0.9425847]\n",
            "this movie was not bad\n",
            "\n",
            "Prediction: [0.3895842]\n",
            "i loved nothing in this movie\n",
            "\n",
            "Prediction: [0.99807334]\n",
            "i loved everything in this movie\n",
            "\n",
            "Prediction: [0.02199166]\n",
            "i hated nothing in this movie\n",
            "\n",
            "Prediction: [0.00469216]\n",
            "i hated everything in this movie\n",
            "\n",
            "Prediction: [0.54437417]\n",
            "tom thought this was a great movie, i disagree with him\n",
            "\n",
            "Prediction: [0.95732653]\n",
            "tom thought this was a great movie, i agree with him\n",
            "\n",
            "Prediction: [0.93373185]\n",
            "tom thought this was a great movie\n",
            "\n",
            "Prediction: [0.1339599]\n",
            "this was not my favorite. the story was good but the graphics left something to be desired\n",
            "\n",
            "Prediction: [0.9553451]\n",
            "it seems like every time i go to the theater this movie has the most people. when i finally decided to go watch it, i realized what i was missing out on\n",
            "\n",
            "Prediction: [0.9987208]\n",
            "this was my favorite. the story was good and the graphics were a force to be reckoned with\n",
            "\n",
            "Prediction: [0.2697872]\n",
            "people have said this was not so bad, they were not kidding\n",
            "\n",
            "Prediction: [0.6917707]\n",
            "this movie was okay but i prefer something else\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}